import streamlit as st

# DEVE SER O PRIMEIRO COMANDO STREAMLIT
st.set_page_config(
    page_title="VIAQUEST Insights (Sales)", 
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Agora importe os outros m√≥dulos
import json
import os
from cache_db import save_interaction, log_error, get_user_history
from config import MAX_RATE_LIMIT, DATASET_ID, PROJECT_ID, TABLES_CONFIG  # Importa a configura√ß√£o do assistente
from style import MOBILE_IFRAME_BASE  # Importa o m√≥dulo de estilos
from gemini_handler import initialize_model, refine_with_gemini, should_reuse_data
from database import build_query, execute_query
from utils import display_message_with_spoiler, slugfy_response
from rate_limit import RateLimiter
from logger import log_interaction

def safe_serialize_gemini_params(params):
    """
    Serializa par√¢metros do Gemini de forma segura, lidando com RepeatedComposite e outros tipos
    """
    if params is None:
        return None
        
    serializable = {}
    
    for key, value in params.items():
        try:
            # Tenta serializar diretamente primeiro
            json.dumps(value)
            serializable[key] = value
        except (TypeError, ValueError):
            # Se falhar, converte para tipos b√°sicos
            if hasattr(value, '__iter__') and not isinstance(value, (str, bytes)):
                # √â uma lista/sequ√™ncia
                serializable[key] = list(value)
            else:
                # Converte para string como fallback
                serializable[key] = str(value)
    
    return serializable

def safe_serialize_data(data):
    """
    Serializa dados de forma segura para JSON
    """
    if data is None:
        return None
        
    if isinstance(data, list):
        return [
            {
                k: str(v) if not isinstance(v, (str, int, float, bool, type(None))) else v
                for k, v in item.items()
            }
            for item in data
        ]
    
    return data

# ====================================================================
# REUTILIZA√á√ÉO ULTRA-CONSERVADORA DE DADOS
# ====================================================================
# 
# DECIS√ÉO BASEADA EM IA (Gemini) üß† - MODO CONSERVADOR:
# O Gemini analisa o contexto e decide se pode reutilizar dados, mas
# com uma abordagem EXTREMAMENTE conservadora para evitar problemas.
#
# ‚úÖ REUTILIZAR APENAS (casos √≥bvios de exporta√ß√£o/visualiza√ß√£o):
# - "gere um Excel desses dados" ‚Üí REUTILIZA (exporta√ß√£o simples)
# - "criar gr√°fico desses dados" ‚Üí REUTILIZA (visualiza√ß√£o simples)  
# - "mostrar em tabela HTML" ‚Üí REUTILIZA (formata√ß√£o simples)
# - "mais detalhes sobre esses resultados" ‚Üí REUTILIZA (elabora√ß√£o simples)
#
# ‚ùå NOVA CONSULTA SEMPRE (casos que requerem SQL):
# - "compare com 2024" ‚Üí NOVA CONSULTA (dados diferentes)
# - "mostre tamb√©m SP" ‚Üí NOVA CONSULTA (filtro adicional)
# - "calcule a porcentagem" ‚Üí NOVA CONSULTA (deixa SQL calcular)
# - "qual modelo vendeu mais?" ‚Üí NOVA CONSULTA (pode n√£o estar nos dados)
# - "some com janeiro" ‚Üí NOVA CONSULTA (agrega√ß√£o)
# - Qualquer manipula√ß√£o, agrega√ß√£o, compara√ß√£o, filtro adicional
#
# üî¥ FILOSOFIA: EM CASO DE D√öVIDA, SEMPRE NOVA CONSULTA!
# Melhor fazer SQL otimizado do que manipular dados localmente.
# Isso garante precis√£o e evita complexidade desnecess√°ria.
# ====================================================================

# Configura√ß√£o do rate limit (100 requisi√ß√µes por dia)
rate_limiter = RateLimiter(max_requests_per_day=MAX_RATE_LIMIT)
#Inicializa vari√°veis para armazenar os dados
refined_response = None
serializable_params = None
serializable_data = None
tech_details = None
query = None
# Vari√°vel para controlar a exibi√ß√£o de detalhes t√©cnicos
SHOW_TECHNICAL_SPOILER = True  # Defina como True para mostrar detalhes t√©cnicos

# Configura√ß√£o de estilos para mobile
st.markdown(MOBILE_IFRAME_BASE, unsafe_allow_html=True)

if "last_data" not in st.session_state:
    st.session_state.last_data = {
        "raw_data": None,
        "params": None,
        "query": None,
        "tech_details": None,
        "prompt": None,
        "df": None  # Novo: DataFrame para exporta√ß√£o
    }

# Carrega as credenciais do arquivo
with open(os.path.join(os.path.dirname(__file__), "credentials.json"), "r") as f:
    creds = json.load(f)

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.title("Login VIAQUEST Insights")
    login = st.text_input("E-mail", value="", key="login_input")
    password = st.text_input("Senha", type="password", key="password_input")
    if st.button("Entrar"):
        if login == creds["login"] and password == creds["password"]:
            st.session_state.authenticated = True
            st.rerun()
        else:
            st.error("Usu√°rio ou senha inv√°lidos.")
    st.stop()

# Container principal para todo o conte√∫do
with st.container():
    
    st.title("VIAQUEST Insights (Sales) - Agentes de IA para a √°rea Comercial")

    with st.expander("‚ö†Ô∏è Limita√ß√µes e Regras do Assistente (clique para ver)", expanded=False):
        st.markdown(
            f"""
            - Este assistente **s√≥ pode consultar a tabela de vendas de ve√≠culos** configurada no sistema.
            - **N√£o √© poss√≠vel acessar ou cruzar dados de outras tabelas** ou fontes externas.
            - **Apenas uma consulta por vez** √© permitida. N√£o √© poss√≠vel realizar m√∫ltiplas buscas simult√¢neas.
            - Para compara√ß√µes temporais, utilize perguntas claras (ex: "Compare as vendas de 2023 e 2024 por m√™s").
            - O modelo pode n√£o compreender perguntas muito vagas ou fora do escopo dos dados dispon√≠veis.
            - Resultados s√£o sempre baseados nos dados mais recentes dispon√≠veis na tabela.
            - **Limite di√°rio de requisi√ß√µes: {MAX_RATE_LIMIT}**. Se atingido, voc√™ receber√° uma mensagem de aviso.
            > Para detalhes t√©cnicos, consulte a documenta√ß√£o ou o spoiler abaixo.
            """
        )

    # Exemplos de perguntas (restaurado)
    if "chat_history" not in st.session_state or len(st.session_state.chat_history) == 0:
        st.write("Fa√ßa perguntas sobre vendas de ve√≠culos. Exemplos:")
        st.code(
            """- Qual o total vendido em 2024?
- Compare as vendas entre os meses existentes de 2023 e 2024. 
- Demonstre os modelos vendidos no ceara em 2023?
"""
        )

    # Inicializa√ß√£o do modelo e estado da sess√£o
    if "model" not in st.session_state:
        st.session_state.model = initialize_model()

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    if "last_data" not in st.session_state:
        st.session_state.last_data = {
            "raw_data": None,
            "params": None,
            "query": None,
            "tech_details": None,
            "prompt": None,
        }

    # Exibe o hist√≥rico de chat
    for msg in st.session_state.chat_history:
        display_message_with_spoiler(
            msg["role"], msg["content"], msg.get("tech_details"), SHOW_TECHNICAL_SPOILER
        )

# Container fixo para o input (fora do content-container)
st.markdown('<div class="fixed-bottom">', unsafe_allow_html=True)
prompt = st.chat_input("Fa√ßa sua pergunta...", key="mobile_input")
st.markdown('</div>', unsafe_allow_html=True)

# Captura novo input
if prompt:
    # Verifica o rate limit antes de processar
    if rate_limiter.check_limit():
        st.session_state.chat_history.append({
            "role": "assistant",
            "content": "Limite di√°rio de requisi√ß√µes atingido. Tente novamente amanh√£."
        })
        st.rerun()
    else:
        # Incrementa o contador
        rate_limiter.increment()
        # Adiciona a pergunta ao hist√≥rico
        st.session_state.chat_history.append({"role": "user", "content": prompt})

        try:
            # Busca hist√≥rico do usu√°rio para contexto na decis√£o de reutiliza√ß√£o
            user_history = get_user_history(creds["login"])
            
            # Verifica se deve reutilizar dados usando intelig√™ncia do Gemini
            # O Gemini analisa o contexto completo e decide se os dados existentes s√£o suficientes
            should_reuse = False
            if st.session_state.last_data["raw_data"] is not None:
                reuse_decision = should_reuse_data(
                    st.session_state.model,
                    prompt,
                    st.session_state.last_data,
                    user_history
                )
                should_reuse = reuse_decision.get("should_reuse", False)
            
            if should_reuse:
                # Reutiliza os dados da √∫ltima consulta baseado na decis√£o do Gemini
                with st.spinner("Processando com dados anteriores..."):
                    # Usa os dados j√° dispon√≠veis
                    serializable_data = safe_serialize_data(st.session_state.last_data["raw_data"])
                    
                    refined_response, tech_details = refine_with_gemini(
                        prompt,
                        serializable_data,
                        st.session_state.last_data["params"],
                        st.session_state.last_data["query"],
                    )
                    
                    # Adiciona informa√ß√£o sobre reutiliza√ß√£o nos detalhes t√©cnicos
                    if tech_details:
                        tech_details["reuse_info"] = {
                            "reused": True,
                            "reason": reuse_decision.get("reason", "Decis√£o inteligente do Gemini"),
                            "original_prompt": st.session_state.last_data["prompt"]
                        }

                # Atualiza o hist√≥rico
                st.session_state.chat_history.append(
                    {
                        "role": "assistant",
                        "content": refined_response,
                        "tech_details": tech_details,
                    }
                )

                # Salva a intera√ß√£o de reutiliza√ß√£o no cache
                try:
                    save_interaction(
                        user_id=creds["login"],
                        question=prompt,
                        function_params=safe_serialize_gemini_params(st.session_state.last_data["params"]),
                        query_sql=st.session_state.last_data["query"],
                        raw_data=serializable_data,
                        raw_response=None,
                        refined_response=refined_response,
                        tech_details=tech_details,
                        status="OK",
                        reused_from=st.session_state.last_data.get("prompt")
                    )
                except Exception as cache_error:
                    print(f"Erro ao salvar no cache (reutiliza√ß√£o): {cache_error}")
                    
            else:
                # Processa uma nova consulta
                convo = st.session_state.model.start_chat(
                    history=[
                        {"role": m["role"], "parts": [m["content"]]}
                        for m in st.session_state.chat_history
                        if m["role"] != "assistant" or not m.get("tech_details")
                    ]
                )

                # Mostra que est√° processando
                processing_msg = st.empty()
                processing_msg.chat_message("assistant").markdown(
                    "Processando sua solicita√ß√£o..."
                )

                response = convo.send_message(prompt)

                # Verifica se h√° chamada de fun√ß√£o
                if (
                    response.candidates
                    and response.candidates[0].content.parts[0].function_call
                ):
                        function_call = response.candidates[0].content.parts[0].function_call
                        params = function_call.args

                        # Serializa√ß√£o SEGURA dos par√¢metros usando fun√ß√£o especializada
                        serializable_params = safe_serialize_gemini_params(params)

                        # Obter nome da tabela e construir full_table_id
                        table_name = serializable_params.get("table_name")
                        if table_name not in TABLES_CONFIG.keys():
                            st.error(f"Tabela {table_name} n√£o configurada")
                            st.stop()
                            
                        full_table_id = f"{PROJECT_ID}.{DATASET_ID}.{table_name}"
                        
                        # Construir e executar query
                        query = build_query(full_table_id, serializable_params)
                        raw_data = execute_query(query)

                        if "error" in raw_data:
                            st.session_state.chat_history.append(
                                {
                                    "role": "assistant",
                                    "content": f"Erro na consulta:\n{raw_data['error']}\n\nQuery:\n```sql\n{raw_data['query']}\n```",
                                }
                            )
                        else:
                            # Converte os dados de retorno para um formato serializ√°vel SEGURO
                            serializable_data = safe_serialize_data(raw_data)

                            # Atualiza a mensagem de processamento
                            processing_msg.chat_message("assistant").markdown(
                                "Dados recebidos. Calculando resultados..."
                            )

                            # Refina a resposta com o Gemini
                            refined_response, tech_details = refine_with_gemini(
                                prompt, serializable_data, serializable_params, query
                            )

                            # Atualiza o hist√≥rico e os √∫ltimos dados
                            st.session_state.last_data = {
                                "raw_data": serializable_data,
                                "params": serializable_params,
                                "query": query,
                                "tech_details": tech_details,
                                "prompt": prompt,
                            }

                            # Salva a intera√ß√£o no cache
                            try:
                                save_interaction(
                                    user_id=creds["login"],
                                    question=prompt,
                                    function_params=serializable_params,
                                    query_sql=query,
                                    raw_data=serializable_data,
                                    raw_response=None,  # Ser√° definido abaixo
                                    refined_response=refined_response,
                                    tech_details=tech_details,
                                    status="OK"
                                )
                            except Exception as cache_error:
                                print(f"Erro ao salvar no cache (nova consulta): {cache_error}")

                            # Remove a mensagem de processamento e adiciona a resposta final
                            processing_msg.empty()
                            st.session_state.chat_history.append(
                                {
                                    "role": "assistant",
                                    "content": slugfy_response(refined_response),
                                    "tech_details": tech_details,
                                }
                            )
                else:
                    # Resposta direta sem chamada de fun√ß√£o
                    processing_msg.empty()
                    st.session_state.chat_history.append(
                        {"role": "assistant", "content": response.text}
                    )
                
            # Inicializa vari√°veis para o log (caso de nova consulta sem function call)
            if 'serializable_params' not in locals():
                serializable_params = None
            if 'query' not in locals():
                query = None
            if 'serializable_data' not in locals():
                serializable_data = None
            if 'refined_response' not in locals():
                refined_response = None
            if 'tech_details' not in locals():
                tech_details = None
                    
            # Regra para a estranha manipula√ß√£o de response por parte do gemini
            try:
                if 'response' in locals():
                    raw_response = response.text
                else:
                    raw_response = None
            except (AttributeError, ValueError):
                raw_response = None

            # For√ßa atualiza√ß√£o da tela
            log_interaction(
                user_input=prompt,
                function_params=serializable_params,
                query=query if query else None,
                raw_data=serializable_data if serializable_data else None,
                raw_response=raw_response,
                refined_response=refined_response,
                first_ten_table_lines=serializable_data[:10] if serializable_data else None,
                graph_data=tech_details.get("chart_info")  if tech_details and tech_details.get("chart_info") else None,
                export_data=tech_details.get("export_info") if tech_details and tech_details.get("export_info") else None,  # Preencha se houver exporta√ß√£o de dados
                status="OK",
                status_msg=f"Consulta processada com sucesso.",
                client_request_count=rate_limiter.state["count"],
                custom_fields=None,  # Use se quiser logar algo extra
            )
            st.rerun()

        except Exception as e:
            # Inicializa vari√°veis para o log em caso de erro
            if 'serializable_params' not in locals():
                serializable_params = None
            if 'query' not in locals():
                query = None
            if 'serializable_data' not in locals():
                serializable_data = None
            if 'raw_response' not in locals():
                raw_response = None
            if 'refined_response' not in locals():
                refined_response = None
            if 'tech_details' not in locals():
                tech_details = None
                
            log_interaction(
                user_input=prompt,
                function_params=serializable_params,
                query=query if query else None,
                raw_data=serializable_data if serializable_data else None,
                raw_response=raw_response,
                refined_response=refined_response if refined_response else None,
                first_ten_table_lines=None,
                graph_data=tech_details.get("chart_info") if tech_details and tech_details.get("chart_info") else None,
                export_data=tech_details.get("export_info") if tech_details and tech_details.get("export_info") else None,
                status="ERROR",
                status_msg=str(e),
                client_request_count=rate_limiter.state["count"],
                custom_fields=None,
            )
            st.session_state.chat_history.append(
                {"role": "assistant", "content": f"Ocorreu um erro: {str(e)}"}
            )
            st.rerun()